from typing import Optional, Dict, Any, List
from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks
from pydantic import BaseModel, Field
import asyncio
import uuid
from datetime import datetime

from api.schemas.chat_schemas import (
    ChatRequest, ChatResponse, ChatSession, StreamingChatRequest
)
from services.orchestrator.orchestrator_service import OrchestratorService
from services.llm.provider_manager import llm_provider_manager
from config.settings import get_settings
from utils.logging import get_logger

router = APIRouter(prefix="/chat", tags=["chat"])
logger = get_logger(__name__)
settings = get_settings()

# Initialize orchestrator service globally
orchestrator_service = OrchestratorService()

@router.post("/", response_model=ChatResponse)
async def chat_endpoint(
    request: ChatRequest,
    background_tasks: BackgroundTasks
) -> ChatResponse:
    """
    üéØ Main Chat Endpoint v·ªõi Intelligent Orchestrator
    
    Thay th·∫ø keyword-based routing b·∫±ng LLM orchestrator
    """
    
    session_id = request.session_id or str(uuid.uuid4())
    start_time = datetime.now()
    
    logger.info(f"üí¨ Chat request received - Session: {session_id}")
    logger.info(f"üîç Query: {request.query[:100]}...")
    
    try:
        # Prepare user context for orchestrator
        user_context = {
            "conversation_history": request.conversation_history or [],
            "preferences": request.user_preferences or {},
            "session_info": {
                "session_id": session_id,
                "language": request.language,
                "start_time": start_time.isoformat()
            }
        }
        
        # Check orchestrator enabled in admin settings
        if getattr(settings, 'ENABLE_INTELLIGENT_ORCHESTRATOR', True):
            # üß† Use Intelligent Orchestrator (LLM-based agent selection)
            logger.info("üß† Using Intelligent Orchestrator for agent selection")
            
            orchestrated_response = await orchestrator_service.orchestrate_query(
                query=request.query,
                language=request.language,
                user_context=user_context,
                session_id=session_id
            )
            
            # Extract orchestrated results
            response_content = orchestrated_response["response"]
            citations = orchestrated_response.get("citations", [])
            metadata = orchestrated_response.get("metadata", {})
            
            # Log orchestration results
            orchestration_method = metadata.get("orchestration_method", "unknown")
            selected_agents = metadata.get("selected_agents", [])
            
            logger.info(f"‚úÖ Orchestration completed using: {orchestration_method}")
            logger.info(f"üéØ Selected agents: {selected_agents}")
            
        else:
            # üîÑ Fallback to simple LLM response (no orchestration)
            logger.info("üîÑ Orchestrator disabled, using simple LLM response")
            
            response_content, citations, metadata = await _get_simple_llm_response(
                request.query, request.language, user_context
            )
        
        # Calculate execution time
        execution_time = (datetime.now() - start_time).total_seconds()
        
        # Create response
        chat_response = ChatResponse(
            response=response_content,
            session_id=session_id,
            citations=citations,
            language=request.language,
            metadata={
                "execution_time": execution_time,
                "timestamp": datetime.now().isoformat(),
                "query_length": len(request.query),
                **metadata
            }
        )
        
        # Background task: Save conversation history
        if getattr(settings, 'ENABLE_CONVERSATION_HISTORY', True):
            background_tasks.add_task(
                _save_conversation_history,
                session_id=session_id,
                user_query=request.query,
                bot_response=response_content,
                metadata=chat_response.metadata
            )
        
        logger.info(f"‚úÖ Chat response completed in {execution_time:.2f}s")
        return chat_response
        
    except Exception as e:
        logger.error(f"‚ùå Chat endpoint error: {e}")
        
        # Generate error response
        error_response = await _get_error_response(
            request.query, request.language, str(e), session_id
        )
        
        return error_response

async def _get_simple_llm_response(
    query: str, 
    language: str, 
    user_context: Dict[str, Any]
) -> tuple[str, List[str], Dict[str, Any]]:
    """
    Simple LLM response without orchestration
    Fallback when orchestrator is disabled
    """
    
    try:
        # Get primary LLM provider
        enabled_providers = getattr(settings, 'enabled_providers', [])
        if not enabled_providers:
            raise ValueError("No LLM providers enabled in admin settings")
            
        llm = await llm_provider_manager.get_provider(enabled_providers[0])
        
        # Simple prompt based on language
        language_prompts = {
            "vi": f"""
B·∫°n l√† AI Assistant th√¥ng minh v√† h·ªØu √≠ch. H√£y tr·∫£ l·ªùi c√¢u h·ªèi sau m·ªôt c√°ch ch√≠nh x√°c v√† chi ti·∫øt:

C√¢u h·ªèi: {query}

H√£y cung c·∫•p th√¥ng tin h·ªØu √≠ch v√† th·ª±c t·∫ø. N·∫øu kh√¥ng ch·∫Øc ch·∫Øn v·ªÅ th√¥ng tin, h√£y n√≥i r√µ ƒëi·ªÅu ƒë√≥.
""",
            "en": f"""
You are an intelligent and helpful AI Assistant. Please answer the following question accurately and in detail:

Question: {query}

Provide helpful and factual information. If uncertain about information, please clearly state that.
""",
            "ja": f"""
„ÅÇ„Å™„Åü„ÅØÁü•ÁöÑ„ÅßÂΩπÁ´ã„Å§AI„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Åß„Åô„ÄÇ‰ª•‰∏ã„ÅÆË≥™Âïè„Å´Ê≠£Á¢∫„Åã„Å§Ë©≥Á¥∞„Å´Á≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑÔºö

Ë≥™Âïè: {query}

ÂΩπÁ´ã„Å§‰∫ãÂÆü„Å´Âü∫„Å•„ÅèÊÉÖÂ†±„ÇíÊèê‰æõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇÊÉÖÂ†±„Åå‰∏çÁ¢∫ÂÆü„Å™Â†¥Âêà„ÅØ„ÄÅ„Åù„Çå„ÇíÊòéÁ¢∫„Å´Ëø∞„Åπ„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
""",
            "ko": f"""
ÎãπÏã†ÏùÄ ÏßÄÎä•Ï†ÅÏù¥Í≥† ÎèÑÏõÄÏù¥ ÎêòÎäî AI Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏ÏûÖÎãàÎã§. Îã§Ïùå ÏßàÎ¨∏Ïóê Ï†ïÌôïÌïòÍ≥† ÏûêÏÑ∏Ìûà ÎãµÎ≥ÄÌï¥ Ï£ºÏÑ∏Ïöî:

ÏßàÎ¨∏: {query}

ÎèÑÏõÄÏù¥ ÎêòÍ≥† ÏÇ¨Ïã§Ï†ÅÏù∏ Ï†ïÎ≥¥Î•º Ï†úÍ≥µÌï¥ Ï£ºÏÑ∏Ïöî. Ï†ïÎ≥¥Í∞Ä Î∂àÌôïÏã§Ìïú Í≤ΩÏö∞ Î™ÖÌôïÌûà ÎßêÏîÄÌï¥ Ï£ºÏÑ∏Ïöî.
"""
        }
        
        prompt = language_prompts.get(language, language_prompts["vi"])
        
        # Get LLM response
        response = await llm.ainvoke(prompt)
        
        return (
            response.content,
            ["General AI Knowledge"],  # Simple citation
            {
                "method": "simple_llm",
                "provider": enabled_providers[0],
                "confidence": 0.75
            }
        )
        
    except Exception as e:
        logger.error(f"Simple LLM response failed: {e}")
        
        # Last resort fallback
        fallback_messages = {
            "vi": "Xin l·ªói, t√¥i kh√¥ng th·ªÉ x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n l√∫c n√†y. Vui l√≤ng th·ª≠ l·∫°i sau.",
            "en": "Sorry, I cannot process your question at this time. Please try again later.",
            "ja": "Áî≥„ÅóË®≥„ÅÇ„Çä„Åæ„Åõ„Çì„Åå„ÄÅÁèæÂú®„ÅîË≥™Âïè„ÇíÂá¶ÁêÜ„Åß„Åç„Åæ„Åõ„Çì„ÄÇÂæå„Åß„ÇÇ„ÅÜ‰∏ÄÂ∫¶„ÅäË©¶„Åó„Åè„Å†„Åï„ÅÑ„ÄÇ",
            "ko": "Ï£ÑÏÜ°Ìï©ÎãàÎã§. ÌòÑÏû¨ ÏßàÎ¨∏ÏùÑ Ï≤òÎ¶¨Ìï† Ïàò ÏóÜÏäµÎãàÎã§. ÎÇòÏ§ëÏóê Îã§Ïãú ÏãúÎèÑÌï¥ Ï£ºÏÑ∏Ïöî."
        }
        
        return (
            fallback_messages.get(language, fallback_messages["vi"]),
            [],
            {"method": "fallback", "error": str(e), "confidence": 0.1}
        )

async def _get_error_response(
    query: str,
    language: str, 
    error_message: str,
    session_id: str
) -> ChatResponse:
    """Generate error response for failed chat requests"""
    
    error_messages = {
        "vi": f"Xin l·ªói, ƒë√£ x·∫£y ra l·ªói khi x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n: {error_message}",
        "en": f"Sorry, an error occurred while processing your question: {error_message}",
        "ja": f"Áî≥„ÅóË®≥„ÅÇ„Çä„Åæ„Åõ„Çì„Åå„ÄÅ„ÅîË≥™Âïè„ÅÆÂá¶ÁêÜ‰∏≠„Å´„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: {error_message}",
        "ko": f"Ï£ÑÏÜ°Ìï©ÎãàÎã§. ÏßàÎ¨∏ Ï≤òÎ¶¨ Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {error_message}"
    }
    
    return ChatResponse(
        response=error_messages.get(language, error_messages["vi"]),
        session_id=session_id,
        citations=[],
        language=language,
        metadata={
            "error": True,
            "error_message": error_message,
            "timestamp": datetime.now().isoformat(),
            "confidence": 0.0
        }
    )

async def _save_conversation_history(
    session_id: str,
    user_query: str,
    bot_response: str,
    metadata: Dict[str, Any]
) -> None:
    """
    Background task to save conversation history
    Implement database storage logic here
    """
    
    try:
        # Implement conversation history storage
        # This could integrate with PostgreSQL, Redis, or other storage
        
        conversation_record = {
            "session_id": session_id,
            "user_query": user_query,
            "bot_response": bot_response,
            "metadata": metadata,
            "timestamp": datetime.now().isoformat()
        }
        
        logger.info(f"üíæ Saving conversation history for session: {session_id}")
        
        # TODO: Implement actual database storage
        # await conversation_db.save_history(conversation_record)
        
    except Exception as e:
        logger.error(f"‚ùå Failed to save conversation history: {e}")

@router.get("/health")
async def chat_health_check():
    """Health check for chat service"""
    
    try:
        # Check orchestrator service
        orchestrator_status = "enabled" if getattr(settings, 'ENABLE_INTELLIGENT_ORCHESTRATOR', True) else "disabled"
        
        # Check available agents
        available_agents = list(orchestrator_service.agents.keys())
        
        # Check enabled providers
        enabled_providers = getattr(settings, 'enabled_providers', [])
        
        return {
            "status": "healthy",
            "orchestrator": orchestrator_status,
            "available_agents": available_agents,
            "enabled_providers": enabled_providers,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@router.post("/stream", response_model=Dict[str, Any])
async def streaming_chat_endpoint(request: StreamingChatRequest):
    """
    üåä Streaming Chat Endpoint v·ªõi Orchestrator Support
    """
    
    # Note: Implement streaming support for orchestrator in future
    # For now, redirect to regular chat endpoint
    
    logger.info("üåä Streaming chat requested, using regular orchestrated chat")
    
    # Convert streaming request to regular chat request
    chat_request = ChatRequest(
        query=request.query,
        language=request.language,
        session_id=request.session_id,
        conversation_history=request.conversation_history,
        user_preferences=request.user_preferences
    )
    
    # Use regular orchestrated chat
    response = await chat_endpoint(chat_request, BackgroundTasks())
    
    return {
        "response": response.response,
        "session_id": response.session_id,
        "citations": response.citations,
        "metadata": {
            **response.metadata,
            "streaming": False,
            "note": "Streaming support for orchestrator coming soon"
        }
    } 